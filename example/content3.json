{
  "contents": [
    {
      "time_from": "09:00:00",
      "time_to": "09:01:30",
      "user": "李明（后端架构师）",
      "content": {
        "text": "今天我们集中讨论直播系统的技术选型和当前痛点。先同步下背景：最近用户投诉量上升 20%，主要集中在卡顿、延迟和首帧加载慢；同时 Q3 成本报告显示云服务费用超支 15%。请各位从各自视角谈谈问题和建议，重点关注架构优化方向（自研 vs 云服务）、延迟控制、多端兼容这几个核心点。"
      }
    },
    {
      "time_from": "09:01:31",
      "time_to": "09:03:00",
      "user": "王芳（前端负责人）",
      "content": {
        "text": "前端侧的问题最直观。目前 iOS 端用 HTTP-FLV 播放，首帧平均 3.2 秒，安卓用 HLS，首帧 4.5 秒，而用户预期是 2 秒内。另外，Web 端用 Flash 的老方案，Chrome 下崩溃率高达 8%。上周大促直播，有用户反馈切到竖屏时花屏，排查发现是转码后的分辨率适配逻辑没覆盖全面。如果换用 WebRTC，理论上能降延迟，但需要前端重写播放器，且低版本浏览器兼容是个坎。"
      }
    },
    {
      "time_from": "09:03:01",
      "time_to": "09:05:00",
      "user": "张伟（运维主管）",
      "content": {
        "text": "运维压力主要在两点：一是现用某云厂商的直播服务，弹性扩缩容响应慢 —— 上周五晚 8 点的明星直播，峰值并发从 50 万涨到 120 万，扩容用了 25 分钟，期间 5 万用户连不上；二是自研的转码集群，依赖人工监控，上周三因 GPU 节点过热未及时告警，导致 3 场直播断流。如果全自研，需要自建边缘节点，但测算过，覆盖全国 31 省需要新增 200 + 边缘服务器，初期投入至少 800 万。"
      }
    },
    {
      "time_from": "09:05:01",
      "time_to": "09:07:00",
      "user": "陈丽（产品经理）",
      "content": {
        "text": "用户侧反馈最集中的是‘互动延迟’—— 比如主播发红包，用户点击后 3-5 秒才收到，严重影响参与感。另外，海外用户（占比 12%）的卡顿率是国内的 3 倍，主要因为当前 CDN 节点没覆盖东南亚和南美。产品需求上，Q4 要推‘直播 + VR’新功能，需要支持 360° 视频流，现有架构可能扛不住高码率传输。技术选型得考虑未来 6-12 个月的扩展需求。"
      }
    },
    {
      "time_from": "09:07:01",
      "time_to": "09:09:00",
      "user": "赵强（测试组长）",
      "content": {
        "text": "测试发现三个关键问题：1）现有压测工具只能模拟 100 万并发，但上周大促实际到了 150 万，超出部分的断流率达 12%；2）弱网场景（2G/3G）下，iOS 和安卓的降级策略（比如从 1080P 切到 360P）不一致，导致部分用户直接黑屏；3）第三方云服务的日志接口不开放，故障排查时拿不到完整的传输链路数据，每次定位问题要 2 小时以上。如果自研，必须配套完善的测试工具链。"
      }
    },
    {
      "time_from": "09:09:01",
      "time_to": "09:11:00",
      "user": "刘燕（直播业务运营）",
      "content": {
        "text": "运营数据更扎心：近 30 天，卡顿率 > 5% 的直播场次，观众留存率比流畅场次低 40%；延迟 > 2 秒的场次，打赏金额下降 35%。上周做了个 AB 测试，用某新云服务商的‘超低延迟方案’，延迟从 2.8 秒降到 1.2 秒，互动率提升了 22%，但该服务商的价格比现用的高 25%。另外，主播端反馈‘推流工具难用’—— 现在用 OBS 推流，手机主播需要连电脑，而竞品已经支持手机直接推流 H.265，画质更好且节省带宽。"
      }
    },
    {
      "time_from": "09:11:01",
      "time_to": "09:13:00",
      "user": "周浩（数据工程师）",
      "content": {
        "text": "从日志分析看，卡顿的主因 60% 是‘边缘节点带宽不足’，25% 是‘转码延迟’，15% 是‘客户端解码能力弱’。比如，晚上 8-10 点的热门直播间，70% 的卡顿发生在二三线城市的边缘节点，这些节点的带宽使用率长期超 90%。另外，转码环节的平均耗时是 800ms，而行业标杆是 500ms 以内 —— 主要瓶颈在转码服务器的 CPU 利用率，目前平均 75%，但峰值到 95% 就会排队。"
      }
    },
    {
      "time_from": "09:13:01",
      "time_to": "09:15:00",
      "user": "吴敏（音视频技术专家）",
      "content": {
        "text": "技术层面，现有方案用 RTMP 推流 + HTTP-FLV 播放，延迟本身就有 1-3 秒，叠加网络抖动会到 5 秒以上。如果换 WebRTC，理论延迟能降到 500ms 内，但需要解决 NAT 穿透、多路复用的问题，且主播端需要支持 VP8/VP9 编码，部分老设备可能不兼容。另外，转码用 FFmpeg 的默认参数，没针对我们的场景优化 —— 比如，将 H.264 的 Profile 从 Baseline 升到 Main，能节省 20% 带宽，但需要客户端支持，得做兼容性测试。"
      }
    },
    {
      "time_from": "09:15:01",
      "time_to": "09:17:00",
      "user": "李明（后端架构师）",
      "content": {
        "text": "综合各位的反馈，当前痛点集中在‘高延迟、高卡顿、高成本、低扩展’。技术选型有三个方向：1）升级现有云服务套餐（加钱买 SLA 保障 + 边缘节点）；2）部分自研（比如转码模块 + 边缘节点），其余用云服务；3）全自研（从推流到播放全链路）。王芳，前端如果切 WebRTC，开发周期和风险？"
      }
    },
    {
      "time_from": "09:17:01",
      "time_to": "09:18:30",
      "user": "王芳（前端负责人）",
      "content": {
        "text": "WebRTC 前端改造需要 3 个月，主要是播放器重写和浏览器兼容（需要适配 Chrome 80+、Safari 14+）。但好处是能统一 iOS / 安卓 / Web 的播放逻辑，长期看维护成本低。风险是部分用户（约 5%）的手机不支持 H.265 硬解码，可能需要回退到软解，导致耗电增加。"
      }
    },
    {
      "time_from": "09:18:31",
      "time_to": "09:20:00",
      "user": "张伟（运维主管）",
      "content": {
        "text": "如果选方案 2（部分自研），建议先自研边缘节点。我们调研过，用阿里云的 ECS 做边缘节点，单节点成本比云厂商的直播服务低 30%，且能自主控制扩缩容。但需要配套监控系统，比如用 Prometheus+Grafana，这部分开发需要 1 个月。"
      }
    },
    {
      "time_from": "09:20:01",
      "time_to": "09:21:30",
      "user": "陈丽（产品经理）",
      "content": {
        "text": "产品侧支持方案 2，因为全自研周期太长（至少 6 个月），而 Q4 要推 VR 直播，等不及。建议优先解决延迟和卡顿，成本其次。另外，主播端推流工具必须优化，争取 11 月前上线手机直推功能，否则大促主播流失风险高。"
      }
    },
    {
      "time_from": "09:21:31",
      "time_to": "09:23:00",
      "user": "赵强（测试组长）",
      "content": {
        "text": "测试资源得提前规划。如果自研边缘节点，需要模拟全国各地区的网络环境（比如用 Chaos Mesh 注入延迟），这部分测试环境搭建需要 2 周。另外，WebRTC 的兼容性测试要覆盖 50 + 款主流手机，建议提前找第三方测服合作。"
      }
    },
    {
      "time_from": "09:23:01",
      "time_to": "09:24:30",
      "user": "刘燕（直播业务运营）",
      "content": {
        "text": "运营这边可以配合做 AB 测试。比如，选 10 场腰部主播的直播，用新边缘节点方案，对比卡顿率和留存率。另外，需要技术同学输出‘用户可见的优化点’，比如在 APP 内提示‘当前直播延迟 < 1 秒’，提升用户感知。"
      }
    },
    {
      "time_from": "09:24:31",
      "time_to": "09:26:00",
      "user": "周浩（数据工程师）",
      "content": {
        "text": "数据团队可以在 1 周内搭建实时监控看板，重点跟踪边缘节点带宽使用率、转码耗时、各地区卡顿率。另外，需要技术同学开放日志接口，否则无法做深度分析 —— 比如，现在拿不到客户端的网络类型（Wi-Fi/4G），没法针对性优化。"
      }
    },
    {
      "time_from": "09:26:01",
      "time_to": "09:27:30",
      "user": "吴敏（音视频技术专家）",
      "content": {
        "text": "转码优化我可以牵头。计划将 FFmpeg 的转码参数调整为 CRF 23+Preset medium，预计能节省 15% 带宽，同时延迟降低 200ms。另外，针对手机推流，建议用 Licode SDK，支持 H.265 硬件编码，比 OBS 软解效率高 3 倍。"
      }
    },
    {
      "time_from": "09:27:31",
      "time_to": "09:29:00",
      "user": "李明（后端架构师）",
      "content": {
        "text": "总结讨论：优先采用‘部分自研 + 云服务’的混合方案，10 月底前完成边缘节点搭建和转码优化，11 月上线手机直推功能，同步启动 WebRTC 前端改造。下一步分工：张伟负责边缘节点部署，吴敏牵头转码和推流优化，王芳跟进前端改造，周浩搭建监控看板，赵强制定测试方案，陈丽和刘燕同步运营需求。下周三前各小组提交详细计划，我来协调资源。散会。"
      }
    },
        {
          "time_from": "09:29:01",
          "time_to": "09:30:30",
          "user": "张伟（运维主管）",
          "content": {
            "text": "关于边缘节点部署，我再补充几个细节。目前计划优先覆盖卡顿率最高的 15 个二三线城市（比如郑州、东莞、佛山），这些地区占了总卡顿投诉的 40%。节点选址需要靠近运营商机房，比如在郑州选中国联通的 BGP 机房，降低跨网延迟。另外，自研节点的容灾方案得考虑 —— 每个城市至少部署 2 个节点，通过 Anycast 技术做负载均衡，单个节点宕机时流量 3 秒内切换到备用节点。监控系统除了 Prometheus，还需要集成阿里云的云监控，做双链路告警，避免漏报。"
          }
        },
        {
          "time_from": "09:30:31",
          "time_to": "09:32:00",
          "user": "吴敏（音视频技术专家）",
          "content": {
            "text": "转码优化这块，我之前提的 CRF 23 参数需要再验证。昨天用内部直播流做了测试：原参数下 1080P 的码率是 4Mbps，调整后降到 3.4Mbps（省 15%），但主观画质测试中，20 个测试用户有 3 个反馈‘暗场景细节模糊’。可能需要把 CRF 调到 22，码率 3.6Mbps，这样画质损失更小。另外，Licode SDK 集成有个坑 ——iOS 端需要适配 AVFoundation 框架，之前用 OBS 推流时没涉及的麦克风权限、摄像头方向锁问题，现在都要处理。我这边需要前端同事配合，10 月 15 日前完成 SDK 的预集成测试。"
          }
        },
        {
          "time_from": "09:32:01",
          "time_to": "09:33:30",
          "user": "王芳（前端负责人）",
          "content": {
            "text": "WebRTC 的兼容性测试，我查了下用户分布：Chrome 占 65%（80 + 版本占 92%），Safari 占 20%（14 + 版本占 85%），Firefox 占 8%（78 + 版本占 70%）。低版本用户主要是 50 岁以上群体，占比不到 3%。这部分用户可以回退到 HLS 播放，但需要做‘版本检测 + 友好提示’—— 比如弹出‘建议升级浏览器获得更流畅体验’的 Toast。另外，iOS 端 WebRTC 播放器需要和原生 APP 深度集成，可能需要客户端团队开放部分接口，我已经同步给张磊（客户端负责人），他说 10 月 10 日前提供接口文档。"
          }
        },
        {
          "time_from": "09:33:31",
          "time_to": "09:35:00",
          "user": "赵强（测试组长）",
          "content": {
            "text": "测试方案我大概理了下：边缘节点测试分三阶段 ——10 月 10 日前做单节点压力测试（模拟 10 万并发，验证带宽和延迟）；10 月 20 日前做多节点容灾测试（模拟断网 / 宕机，验证切换时间）；10 月 25 日前做全国混网测试（用 Chaos Mesh 模拟 2G/3G / 高延迟网络，测卡顿率）。WebRTC 测试需要覆盖：1）50 款手机（包括华为 Mate40、小米 12、iPhone 12 等主流机型）；2）弱网场景（丢包率 10%、延迟 200ms）；3）多任务切换（切到后台再切回，验证重连时间）。第三方测服我联系了 Testin，他们能提供真机云服务，报价 3 万 / 月，需要提前申请预算。"
          }
        },
        {
          "time_from": "09:35:01",
          "time_to": "09:36:30",
          "user": "陈丽（产品经理）",
          "content": {
            "text": "VR 直播的技术需求得提前对齐。Q4 要上线的是‘360° 全景 + 720P’，码率预计 8-10Mbps，现有转码集群的单节点处理能力是 50 路，按峰值 1000 场 VR 直播算，需要 200 路并发转码，当前集群只有 80 路，得在 11 月底前扩容 120 路。另外，VR 播放器需要支持陀螺仪控制视角，前端得和客户端团队确认，是否需要开发专用的 VR 播放组件 —— 这个需求我已经加到 PRD 里，10 月 15 日前输出详细规格。"
          }
        },
        {
          "time_from": "09:36:31",
          "time_to": "09:38:00",
          "user": "刘燕（直播业务运营）",
          "content": {
            "text": "AB 测试的具体场次我和运营团队对过：选 10 场腰部主播（粉丝量 50 万 - 200 万），覆盖美妆、游戏、美食三个品类，时间分布在晚上 7-10 点（用户活跃高峰）。需要技术同学在播放器里埋两个埋点 ——‘进入直播间时的延迟值’和‘观看过程中卡顿次数’。另外，用户感知优化的提示语，我准备了两个方案：A 方案‘当前直播延迟 0.8 秒，丝滑无卡顿’；B 方案‘点击查看实时延迟：0.8 秒’。10 月 10 日前需要 UI 同学出视觉稿，同步在 APP 内小范围测试哪个更有效。"
          }
        },
        {
          "time_from": "09:38:01",
          "time_to": "09:40:00",
          "user": "周浩（数据工程师）",
          "content": {
            "text": "监控看板的指标我细化了：核心指标包括边缘节点（带宽使用率、CPU / 内存使用率、单节点并发数）、转码集群（任务队列长度、平均耗时、失败率）、播放端（首帧时间、卡顿率、延迟分布）。需要技术同学开放的日志字段有：客户端网络类型（Wi-Fi/4G/5G）、运营商（移动 / 联通 / 电信）、地理位置（精确到市）、设备型号。另外，云服务部分的日志，我联系了现用云厂商的技术支持，他们说可以开放 API，但需要签数据接口协议，法务部需要 1 周时间审核，10 月 12 日前能完成。"
          }
        },
        {
          "time_from": "09:40:01",
          "time_to": "09:41:30",
          "user": "李明（后端架构师）",
          "content": {
            "text": "现在需要确认几个关键风险点：1）自研边缘节点的成本是否真能降 30%？张伟，你之前说单节点成本对比，具体数字是多少？2）WebRTC 改造如果遇到技术瓶颈（比如 NAT 穿透成功率低），有没有备用方案？3）VR 转码扩容的 120 路，是买新服务器还是用云厂商的弹性计算？"
          }
        },
        {
          "time_from": "09:41:31",
          "time_to": "09:43:00",
          "user": "张伟（运维主管）",
          "content": {
            "text": "成本对比：现用云厂商的直播服务，按峰值 100 万并发算，每月费用是 120 万；自研边缘节点用阿里云 c6 实例（8 核 16G），单节点支撑 5 万并发，200 万并发需要 40 个节点，每月费用是 40×1.2 万 = 48 万（含带宽），确实能省 60%。但要注意，自研节点需要额外的运维人力 —— 现在运维团队有 15 人，需要抽调 3 人专职维护，可能需要招聘 1-2 名边缘计算工程师，这部分人力成本每月增加 8 万，综合省 50% 左右。"
          }
        },
        {
          "time_from": "09:43:01",
          "time_to": "09:44:30",
          "user": "吴敏（音视频技术专家）",
          "content": {
            "text": "WebRTC 的 NAT 穿透问题，我们可以用 STUN+TURN 方案。STUN 用于对称 NAT 穿透（成功率 80%），TURN 作为中继（覆盖剩余 20%）。TURN 服务器计划用 coturn 开源方案，部署在阿里云的公网 IP 节点，预计需要 5 个 TURN 服务器，每个支撑 10 万并发。备用方案是，如果 TURN 服务器压力过大，回退到 HTTP-FLV 播放，但会损失延迟优势，这个需要前端做开关控制。"
          }
        },
        {
          "time_from": "09:44:31",
          "time_to": "09:46:00",
          "user": "陈丽（产品经理）",
          "content": {
            "text": "VR 转码扩容，我问了财务，Q4 预算里有 200 万的硬件采购额度。如果买新服务器（每台 8 万，含 GPU），能买 25 台，每台支持 5 路转码，25 台就是 125 路，刚好满足需求。云厂商的弹性计算按小时计费，峰值时每路转码费用是 0.5 元 / 小时，1000 场直播按 2 小时算，单次成本 1000×2×0.5=1000 元，长期看买服务器更划算。"
          }
        },
        {
          "time_from": "09:46:01",
          "time_to": "09:47:30",
          "user": "李明（后端架构师）",
          "content": {
            "text": "风险评估基本可控。接下来明确几个 deadline：10 月 10 日前，张伟提交边缘节点部署方案（含选址、容灾、监控）；吴敏提交转码参数最终方案（附画质测试报告）；王芳提交 WebRTC 改造排期（含客户端接口对接计划）。10 月 15 日前，赵强提交测试方案（含第三方测服合同）；周浩提交监控看板需求文档（含日志字段清单）；刘燕提交 AB 测试方案（含埋点需求）。陈丽同步 VR 转码扩容的采购需求给 IT 部。有问题吗？"
          }
        },
        {
          "time_from": "09:47:31",
          "time_to": "09:48:30",
          "user": "赵强（测试组长）",
          "content": {
            "text": "测试环境搭建需要服务器资源，现在测试机房只有 50 台机器，模拟 150 万并发不够。需要申请 100 台阿里云弹性 ECS，10 月 8 日前到位，否则 10 月 10 日的单节点压力测试做不了。"
          }
        },
        {
          "time_from": "09:48:31",
          "time_to": "09:49:30",
          "user": "李明（后端架构师）",
          "content": {
            "text": "资源申请我来协调，今天下班前批给你。还有其他问题吗？"
          }
        },
        {
          "time_from": "09:50:01",
          "time_to": "09:51:00",
          "user": "李明（后端架构师）",
          "content": {
            "text": "那今天的会就到这里，各小组按分工推进，有问题随时在‘直播技术攻坚群’同步。散会。"
          }
        }
  ]
}